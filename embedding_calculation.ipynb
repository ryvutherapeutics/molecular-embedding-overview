{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "921b6444-b562-4478-ae55-8a8a12c8020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from rdkit.Chem import rdFingerprintGenerator as rdGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ec977-8178-4ebd-8b55-4f98c27a6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list = np.genfromtxt(\"smiles.txt\", dtype=str, delimiter='\\n', comments=None)[:5000]\n",
    "print(f\"Loaded compounds: {len(smiles_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beae106-716c-4e54-98c7-ec6c2fc218b2",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0981ea-02b2-4601-b55e-f8f5f20baa30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mol2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc9862",
   "metadata": {},
   "source": [
    "The pre-trained model can be downloaded from [github.com/samoturk/mol2vec/blob/master/examples/models](https://github.com/samoturk/mol2vec/blob/master/examples/models/model_300dim.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48556078-615e-43ec-af42-abcb5a0fd3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mol2vec.features import mol2alt_sentence, MolSentence\n",
    "from gensim.models import word2vec\n",
    "\n",
    "mol2vec_model = word2vec.Word2Vec.load(\"mol2vec_model_300dim.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddd93e",
   "metadata": {},
   "source": [
    "The source of **sentence2vec** function is [github.com/samoturk/mol2vec/issues/14](https://github.com/samoturk/mol2vec/issues/14) and was a solution suggested for the deprecation problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8313cfa-fb06-4854-9e06-64dc745edc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences2vec(sentences, model, unseen=None):    \n",
    "    keys = set(model.wv.key_to_index)\n",
    "    vec = []\n",
    "    if unseen:\n",
    "        unseen_vec = model.wv.get_vector(unseen)\n",
    "    for sentence in sentences:\n",
    "        if unseen:\n",
    "            vec.append(sum([model.wv.get_vector(word) if word in set(sentence) & keys else unseen_vec for word in sentence]))\n",
    "        else:\n",
    "            vec.append(sum([model.wv.get_vector(word) for word in sentence if word in set(sentence) & keys]))\n",
    "    return np.array(vec)\n",
    "\n",
    "def get_mol2vec(smiles_list, model=None):\n",
    "    if model == None:\n",
    "        global mol2vec_model\n",
    "        model = mol2vec_model\n",
    "    sentence_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        sentence = MolSentence(mol2alt_sentence(mol, radius=2))\n",
    "        sentence_list.append(sentence)\n",
    "    vec_list = sentences2vec(sentence_list, model)\n",
    "    return vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "495187b6-bb17-4a83-b33d-96380a55496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_mol2vec(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57d60c-4dc7-4bc5-9c67-2ed58b2debe4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Graph2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3a0a8ab7-37f2-459c-8b65-f1334c53d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from karateclub import Graph2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "989e475b-ba4d-43c0-818c-5af526117be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol2graph(mol):\n",
    "    G = nx.Graph()\n",
    "    for atom in mol.GetAtoms():\n",
    "        G.add_node(\n",
    "            atom.GetIdx(),\n",
    "            atomic_num = atom.GetAtomicNum(),\n",
    "            atom_symbol = atom.GetSymbol()\n",
    "        )  \n",
    "    for bond in mol.GetBonds():\n",
    "        G.add_edge(\n",
    "            bond.GetBeginAtomIdx(),\n",
    "            bond.GetEndAtomIdx(),\n",
    "            bond_type = bond.GetBondType()\n",
    "        )\n",
    "    return G\n",
    "\n",
    "def get_graph2vec(smiles_list, model=None):\n",
    "    if model == None:\n",
    "        model = Graph2Vec()\n",
    "    graph_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        graph = mol2graph(mol)\n",
    "        graph_list.append(graph)\n",
    "    model.fit(graph_list)\n",
    "    vec_list = model.get_embedding()\n",
    "    return vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "76d27494-4aa6-469d-9064-25212c4c3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_graph2vec(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d7b56-b4b6-4a53-aa31-772f2e98792c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ChemBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "964d136d-e63d-4c4f-936f-23f413111db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e06c2-8576-45b7-a916-13209c2e67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chemberta = AutoModelForMaskedLM.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "chemberta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a359a",
   "metadata": {},
   "source": [
    "This wrapper function for using ChemBert2a model was provided on [www.kaggle.com/code/alexandervc/chembert2a-smiles-embeddings-for-beginners](https://www.kaggle.com/code/alexandervc/chembert2a-smiles-embeddings-for-beginners/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "389b545e-4a17-41a9-9dc1-70c9f3e5df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chemberta(smiles_list):\n",
    "    embeddings_cls = torch.zeros(len(smiles_list), 600)\n",
    "    embeddings_mean = torch.zeros(len(smiles_list), 600)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, smiles in enumerate(tqdm(smiles_list)):\n",
    "            encoded_input = tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            model_output = chemberta(**encoded_input)\n",
    "            # embedding = model_output[0][::,0,::]      # class embedding\n",
    "            # embeddings_cls[i] = embedding\n",
    "            embedding = torch.mean(model_output[0], 1)\n",
    "            embeddings_mean[i] = embedding\n",
    "            \n",
    "    return embeddings_mean.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd6a4b3-bcbd-4d37-a180-421920c6cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_chemberta(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3afb326-9a2f-4a3e-b2f2-1476ce63a31b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Continuous Data Driven Descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be780bf-058e-408e-a18d-4ca8b9e2e5f9",
   "metadata": {},
   "source": [
    "The descriptors are generated using the CDDD REST - see [repository](https://github.com/vaxherra/cddd_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9dce8bf4-9d7b-4702-9fba-8c2f6027860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0474621b-65a0-4a45-b793-f33dfdee85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_json(smiles_list, batch_size=1):\n",
    "    batches = [smiles_list[i:i + batch_size] for i in range(0, len(smiles_list), batch_size)]\n",
    "    json_data = {\"batches\": batches}\n",
    "    return json_data\n",
    "\n",
    "def get_descriptors(json_data, url=\"http://192.168.1.100:80/predict\"):  # Replace with address of own container\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    response = requests.post(url, data=json.dumps(json_data), headers=headers)\n",
    "    response_json = response.json()\n",
    "    response_dict = json.loads(response_json[\"Prediction\"])\n",
    "    df = pd.DataFrame.from_dict(response_dict)\n",
    "    descriptor_list = df.iloc[:, 2:].values\n",
    "    return descriptor_list\n",
    "\n",
    "def get_cddd(smiles_list, batch_size=1):\n",
    "    smiles_list = list(smiles_list)\n",
    "    json_data = prepare_json(smiles_list)\n",
    "    descriptor_list = get_descriptors(json_data)\n",
    "    return descriptor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bc0ece9d-ce1b-4a70-a778-7c9d5e8dad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_cddd(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cea816-5e3d-48a6-b834-a2a8730f210d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Molecular Transformer Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b5cdf-5f4d-4234-8dff-b3f8cf73b7d7",
   "metadata": {},
   "source": [
    "The embedding can be calculated using the scripts accessible [here](https://github.com/mpcrlab/MolecularTransformerEmbeddings). Then the generated .npz file can be ocnverted to embedding list using the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb786933-d997-490b-8245-a9a07f1870e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mte(file_path, smiles_list):\n",
    "    embedding_npz = np.load(file_path)\n",
    "    embedding_list = []\n",
    "    for smiles in smiles_list:\n",
    "        embedding = np.mean(embedding_npz[smiles], axis=0)\n",
    "        embedding_list.append(embedding)\n",
    "    embedding_list = np.array(embedding_list)\n",
    "    return embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d4c25d-586f-4bfd-a79d-8d87262eec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_mte(\"mte_embedding.npz\", smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df5454-381a-40b1-9719-5bccd869f7c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MACAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a7e46776-4808-4f7e-8b3e-426eecbdc0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from macaw import MACAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a85136c8-5dcf-4277-9ccb-ad946c37a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macaw(smiles_list, n_dimensions=20):\n",
    "    mcw = MACAW(n_components=n_dimensions)\n",
    "    mcw.fit(smiles_list)\n",
    "    embedding = mcw.transform(smiles_list)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2447c893-8cc9-45a0-9d6d-38998a730dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_macaw(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30519744-3eff-47b1-be99-ed9c57b49e1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MolFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13dc556d-aae8-4f2d-977e-bc203e901946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", deterministic_eval=True, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "970dc050-2f43-4652-ae57-bc76eceb3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molformer(smiles_list):\n",
    "    smiles_list = [str(smiles) for smiles in smiles_list]\n",
    "    inputs = tokenizer(smiles_list, padding=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embedding = np.array(outputs.pooler_output)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "545255a1-33fd-47ed-8e20-0dd397bcd92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_molformer(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e919b7-3345-48af-a345-0f11df37e4c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ad638f-0660-4471-9e8f-713e1e708174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"entropy/gpt2_zinc_87m\", max_len=256)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"entropy/gpt2_zinc_87m\")\n",
    "collator = DataCollatorWithPadding(tokenizer, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbd68da",
   "metadata": {},
   "source": [
    "Embeddings are generated in batches because there are no errors then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05f859a7-c232-4f91-90b7-09fef505698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt2(smiles_list):\n",
    "    smiles_list = [str(smiles) for smiles in smiles_list]\n",
    "    embedding_list = []\n",
    "    for i in range(0, len(smiles_list), 100):\n",
    "        print(f\"{i}:{i+100}\")\n",
    "        inputs = collator(tokenizer(smiles_list[i:i+100]))\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        full_embeddings = outputs[-1][-1]\n",
    "        mask = inputs['attention_mask']\n",
    "        embeddings = ((full_embeddings * mask.unsqueeze(-1)).sum(1) / mask.sum(-1).unsqueeze(-1))\n",
    "        embedding_list.extend(embeddings)\n",
    "    for i, vec in enumerate(embedding_list):\n",
    "        embedding_list[i] = vec.detach().numpy()\n",
    "    return embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751fa95-df9d-4af5-b911-538e1bdcd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_gpt2(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7badf9af-41d6-450d-9377-cf2750a9ff41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## BERT for SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a859abe-f19c-466a-9f7a-c605f1a487fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"unikei/bert-base-smiles\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"unikei/bert-base-smiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ebd01be-dc9a-47b5-9585-26d178daf154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_smiles(smiles_list):\n",
    "    smiles_list = [str(smiles) for smiles in smiles_list]\n",
    "    inputs = tokenizer(smiles_list, padding=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.detach().numpy()\n",
    "    embedding = np.mean(embedding, axis=1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d0fe199-182b-48d0-ade5-c78deca9e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_bert_smiles(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671197c5-5559-4dc3-9539-247f84308e5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff9bee-9868-442c-957d-2e602fdd583b",
   "metadata": {},
   "source": [
    "To use this snippet of code, download the [huggingmolecules repository](https://github.com/gmum/huggingmolecules). Before usage, download the [pretrained weights](https://github.com/gmum/huggingmolecules/blob/main/src/huggingmolecules/models/models_mat.py) and [configuration](https://github.com/gmum/huggingmolecules/blob/main/src/huggingmolecules/configuration/configuration_mat.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1eb409-a0e5-45d3-a8b5-3567e6fd6469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingmolecules.models.models_mat import MatModel\n",
    "from huggingmolecules.configuration.configuration_mat import MatConfig\n",
    "from huggingmolecules.featurization.featurization_mat import MatFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0671222a-54d3-4cfc-9ff3-e298313a6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mat(smiles_list):\n",
    "    # Load config and add missing keys to the state dictionnary\n",
    "    state_dict = torch.load(\"mat_masking_20M.pt\")\n",
    "    missing_keys = (\"generator.proj.weight\", \"generator.proj.bias\")\n",
    "    missing_sizes = ((1, 1024), (1,))\n",
    "    for key, size in zip(missing_keys, missing_sizes):\n",
    "        state_dict[key] = torch.Tensor(np.zeros(size))\n",
    "    config = MatConfig.from_pretrained('mat_masking_20M.json')\n",
    "    \n",
    "    # Load featurizer\n",
    "    featurizer = MatFeaturizer(config)\n",
    "    batch = featurizer(smiles_list[:500])\n",
    "    \n",
    "    # Load model\n",
    "    model = MatModel(config=config)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, featurizer\n",
    "\n",
    "def get_mat(smiles_list):\n",
    "    model, featurizer = load_mat(smiles_list)\n",
    "    batch_mask = torch.sum(torch.abs(batch.node_features), dim=-1) != 0\n",
    "    embedded = model.src_embed(batch.node_features)\n",
    "    encoding = model.encoder(embedded, batch_mask,\n",
    "                           adj_matrix=batch.adjacency_matrix,\n",
    "                           distance_matrix=batch.distance_matrix)\n",
    "    embedding = np.mean(encoding.detach().numpy(), axis=1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c9c5f7-0c1e-430d-a5ca-3045ea880601",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_mat(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2919e0-260f-4e31-9632-5d060743ac7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## R-MAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1282be65-c151-49a8-8947-8f7753a885dd",
   "metadata": {},
   "source": [
    "To use this snippet of code, download the [huggingmolecules repository](https://github.com/gmum/huggingmolecules). Before usage, download the [pretrained weights](https://github.com/gmum/huggingmolecules/blob/main/src/huggingmolecules/models/models_rmat.py) and [configuration](https://github.com/gmum/huggingmolecules/blob/main/src/huggingmolecules/configuration/configuration_rmat.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d63505-6b26-4f2b-b215-9c81c1892e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingmolecules.models.models_rmat import RMatModel\n",
    "from huggingmolecules.configuration.configuration_rmat import RMatConfig\n",
    "from huggingmolecules.featurization.featurization_rmat import RMatFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b736f-1510-4bb5-8c7d-5b9f1736c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rmat(smiles_list):\n",
    "    # Load config and add missing keys to the state dictionnary\n",
    "    state_dict = torch.load(\"rmat_4M.pt\")\n",
    "    missing_keys = (\"generator.att_net.0.weight\", \"generator.att_net.2.weight\", \"generator.proj.weight\", \"generator.proj.bias\")\n",
    "    missing_sizes = ((128, 768), (4, 128), (1, 3072), (1,))\n",
    "    for key, size in zip(missing_keys, missing_sizes):\n",
    "        state_dict[key] = torch.Tensor(np.zeros(size))\n",
    "    config = RMatConfig.from_pretrained('rmat_4M.json')\n",
    "    \n",
    "    # Load featurizer\n",
    "    featurizer = RMatFeaturizer(config)\n",
    "    batch = featurizer(smiles_list)\n",
    "    \n",
    "    # Load model\n",
    "    model = RMatModel(config=config)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model, featurizer\n",
    "\n",
    "def get_rmat(smiles_list):\n",
    "    model, featurizer = load_rmat(smiles_list)\n",
    "    batch_mask = torch.sum(torch.abs(batch.node_features), dim=-1) != 0\n",
    "    embedded = model.src_embed(batch.node_features)\n",
    "    distances_matrix = model.dist_rbf(batch.distance_matrix)\n",
    "    edges_att = torch.cat((batch.bond_features, batch.relative_matrix, distances_matrix), dim=1)\n",
    "    encoding = model.encoder(embedded, batch_mask, edges_att=edges_att)\n",
    "    embedding = np.mean(encoding.detach().numpy(), axis=1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec196a4-2a67-4c52-bf4e-e12976c34bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_rmat(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401bbee-9573-4565-8cb0-f43b82fcc8fc",
   "metadata": {},
   "source": [
    "# Distance and correlation calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f9c76-73a9-4d5e-b3dc-3f49f974981f",
   "metadata": {},
   "source": [
    "Embeddings should be 2-dimensional arrays saved as .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d49b624-d892-48eb-8b0e-b13fb3552d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fingerprints(smiles_list):\n",
    "    fpgen = rdGen.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "    fingerprint_list = np.array([fpgen.GetFingerprint(Chem.MolFromSmiles(smiles)) for smiles in smiles_list], dtype=rdkit.DataStructs.cDataStructs.ExplicitBitVect)\n",
    "    return fingerprint_list\n",
    "\n",
    "def get_pairwise_similarity(smiles_list):    \n",
    "    fingerprint_list = get_fingerprints(smiles_list)\n",
    "    similarity_matrix = 1 - pairwise_distances(X = fingerprint_list, metric='jaccard', n_jobs = -1)\n",
    "    return similarity_matrix\n",
    "\n",
    "def get_pairwise_distance(vec_list, metric):\n",
    "    distance_matrix = pairwise_distances(X=vec_list, metric=metric, n_jobs = -1)\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e302f3-3cdc-4bb2-9671-06646dcab527",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list = np.genfromtxt(\"smiles.txt\", dtype=str, comments=None)\n",
    "print(f\"Loaded {len(smiles_list)} SMILES\")\n",
    "\n",
    "similarity_matrix = get_pairwise_similarity(smiles_list)\n",
    "with open(f\"similarity.pkl\", \"wb\") as file:\n",
    "    pickle.dump(similarity_matrix, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143d32d-0cac-4b60-abbd-60e762e7b97f",
   "metadata": {},
   "source": [
    "### Calculating distances with different measures: euclidean, cosine, Canberra.\n",
    "It is assumed that embedding are saved in *<embedding_name>_embedding.pkl* files. The calculated distance matrices are saved in *distance_<embedding_name>_<measure_name>.pkl*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5254e-8e41-4720-a794-81ade36889cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = (\n",
    "    (\"mol2vec\", get_mol2vec),\n",
    "    (\"graph2vec\", get_graph2vec),\n",
    "    (\"chemberta\", get_chemberta),\n",
    "    (\"cddd\", get_cddd),\n",
    "    (\"mte\", get_mte),\n",
    "    (\"macaw\", get_macaw),\n",
    "    (\"molformer\", get_molformer),\n",
    "    (\"gpt2\", get_mol2vec),\n",
    "    (\"bert_smiles\", get_bert_smiles)\n",
    ")\n",
    "measure_list = (\"euclidean\", \"cosine\", \"canberra\")\n",
    "\n",
    "for emb_name, emb_func in embedding_list:\n",
    "    embedding = emb_func(smiles_list)\n",
    "    embedding = np.nan_to_num(embedding)\n",
    "    print(f\"Calculated {emb_name} embedding: {embedding.shape}\")\n",
    "    with open(f\"{emb_name}_embedding.pkl\", \"wb\") as file:\n",
    "        pickle.dump(similarity_matrix, file)\n",
    "\n",
    "    for m_name in measure_list:\n",
    "        distance_matrix = get_pairwise_distance(embedding, m_name)\n",
    "        with open(f\"distance_{emb_name}_{m_name}.pkl\", \"wb\") as file:\n",
    "            pickle.dump(distance_matrix, file)\n",
    "        print(f\"\\tDistance {m_name} - done.\")\n",
    "    print(f\"Distance {emb_name} - done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df4508-d7af-45c0-a56d-74c5bba4b121",
   "metadata": {},
   "source": [
    "### Calculating correlation between embedding distance and Morgan Tanimoto similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1617de5e-a3d3-418b-97c4-92333f6b7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = (\"mol2vec\", \"graph2vec\", \"chemberta\", \"cddd\", \"mte\", \"macaw\", \"molformer\", \"gpt2\", \"bert_smiles\")\n",
    "measures_list = (\"euclidean\", \"cosine\", \"canberra\")\n",
    "\n",
    "with open(f\"similarity.pkl\", \"rb\") as file:\n",
    "    similarity_matrix = pickle.load(file)\n",
    "print(similarity_matrix.shape)\n",
    "    \n",
    "similarity_matrix_flat = similarity_matrix.flatten()\n",
    "for embedding in embedding_list:\n",
    "    print(f\"{embedding.capitalize()}\")\n",
    "    X = [similarity_matrix_flat]\n",
    "    for measure in measures_list:\n",
    "        with open(f\"distance_{embedding}_{measure}.pkl\", \"rb\") as file:\n",
    "            dist_matrix = pickle.load(file)\n",
    "        dist_matrix = np.nan_to_num(dist_matrix)\n",
    "        dist_matrix = dist_matrix.flatten()\n",
    "        dist_matrix_norm = (dist_matrix - np.min(dist_matrix))/np.ptp(dist_matrix)\n",
    "        X.append(dist_matrix_norm)\n",
    "    corr_matrix = np.corrcoef(X)\n",
    "\n",
    "    for measure, corr in zip(measures_list, corr_matrix[0, 1:]):\n",
    "        print(f\"{measure}: {corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8873d23-a2b2-45e1-b0b6-f45bee852e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install macaw==0.2.dev0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
