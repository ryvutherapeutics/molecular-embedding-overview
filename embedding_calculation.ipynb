{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921b6444-b562-4478-ae55-8a8a12c8020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from rdkit.Chem import rdFingerprintGenerator as rdGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "866ec977-8178-4ebd-8b55-4f98c27a6c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded compounds: 5000\n"
     ]
    }
   ],
   "source": [
    "smiles_list = np.genfromtxt(\"data/jak2_smiles.txt\", dtype=str, delimiter='\\n', comments=None)[:5000]\n",
    "print(f\"Loaded compounds: {len(smiles_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beae106-716c-4e54-98c7-ec6c2fc218b2",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0981ea-02b2-4601-b55e-f8f5f20baa30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mol2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48556078-615e-43ec-af42-abcb5a0fd3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mol2vec.features import mol2alt_sentence, MolSentence\n",
    "from gensim.models import word2vec\n",
    "\n",
    "mol2vec_model = word2vec.Word2Vec.load(\"models/mol2vec_model_300dim.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8313cfa-fb06-4854-9e06-64dc745edc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''https://github.com/samoturk/mol2vec/issues/14'''\n",
    "def sentences2vec(sentences, model, unseen=None):\n",
    "    \"\"\"Generate vectors for each sentence (list) in a list of sentences. Vector is simply a\n",
    "    sum of vectors for individual words.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : list, array\n",
    "        List with sentences\n",
    "    model : word2vec.Word2Vec\n",
    "        Gensim word2vec model\n",
    "    unseen : None, str\n",
    "        Keyword for unseen words. If None, those words are skipped.\n",
    "        https://stats.stackexchange.com/questions/163005/how-to-set-the-dictionary-for-text-analysis-using-neural-networks/163032#163032\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "    \"\"\"\n",
    "    \n",
    "    keys = set(model.wv.key_to_index)\n",
    "    vec = []\n",
    "    \n",
    "    if unseen:\n",
    "        unseen_vec = model.wv.get_vector(unseen)\n",
    "    for sentence in sentences:\n",
    "        if unseen:\n",
    "            vec.append(sum([model.wv.get_vector(word) if word in set(sentence) & keys else unseen_vec for word in sentence]))\n",
    "        else:\n",
    "            vec.append(sum([model.wv.get_vector(word) for word in sentence if word in set(sentence) & keys]))\n",
    "    return np.array(vec)\n",
    "\n",
    "# Main function\n",
    "def get_mol2vec(smiles_list, model=None):\n",
    "    if model == None:\n",
    "        global mol2vec_model\n",
    "        model = mol2vec_model\n",
    "    sentence_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        sentence = MolSentence(mol2alt_sentence(mol, radius=2))\n",
    "        sentence_list.append(sentence)\n",
    "    vec_list = sentences2vec(sentence_list, model)\n",
    "    return vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495187b6-bb17-4a83-b33d-96380a55496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_mol2vec(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57d60c-4dc7-4bc5-9c67-2ed58b2debe4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Graph2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0a8ab7-37f2-459c-8b65-f1334c53d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from karateclub import Graph2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "989e475b-ba4d-43c0-818c-5af526117be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol2graph(mol):\n",
    "    G = nx.Graph()\n",
    "    for atom in mol.GetAtoms():\n",
    "        G.add_node(\n",
    "            atom.GetIdx(),\n",
    "            atomic_num = atom.GetAtomicNum(),\n",
    "            atom_symbol = atom.GetSymbol()\n",
    "        )  \n",
    "    for bond in mol.GetBonds():\n",
    "        G.add_edge(\n",
    "            bond.GetBeginAtomIdx(),\n",
    "            bond.GetEndAtomIdx(),\n",
    "            bond_type = bond.GetBondType()\n",
    "        )\n",
    "    return G\n",
    "\n",
    "def get_graph2vec(smiles_list, model=None):\n",
    "    if model == None:\n",
    "        model = Graph2Vec()\n",
    "    graph_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        graph = mol2graph(mol)\n",
    "        graph_list.append(graph)\n",
    "    model.fit(graph_list)\n",
    "    vec_list = model.get_embedding()\n",
    "    return vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d27494-4aa6-469d-9064-25212c4c3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_graph2vec(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d7b56-b4b6-4a53-aa31-772f2e98792c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ChemBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "964d136d-e63d-4c4f-936f-23f413111db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e06c2-8576-45b7-a916-13209c2e67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chemberta = AutoModelForMaskedLM.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "chemberta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "389b545e-4a17-41a9-9dc1-70c9f3e5df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''https://www.kaggle.com/code/alexandervc/chembert2a-smiles-embeddings-for-beginners/notebook'''\n",
    "def get_chemberta(smiles_list):\n",
    "    embeddings_cls = torch.zeros(len(smiles_list), 600)\n",
    "    embeddings_mean = torch.zeros(len(smiles_list), 600)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, smiles in enumerate(tqdm(smiles_list)):\n",
    "            encoded_input = tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            model_output = chemberta(**encoded_input)\n",
    "            # embedding = model_output[0][::,0,::]\n",
    "            # embeddings_cls[i] = embedding\n",
    "            embedding = torch.mean(model_output[0], 1)\n",
    "            embeddings_mean[i] = embedding\n",
    "            \n",
    "    return embeddings_mean.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4bd6a4b3-bcbd-4d37-a180-421920c6cc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:28<00:00, 176.75it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding = get_chemberta(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3afb326-9a2f-4a3e-b2f2-1476ce63a31b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Continuous Data Driven Descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be780bf-058e-408e-a18d-4ca8b9e2e5f9",
   "metadata": {},
   "source": [
    "The descriptors are generated using the CDDD REST - see [repository](https://github.com/vaxherra/cddd_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dce8bf4-9d7b-4702-9fba-8c2f6027860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0474621b-65a0-4a45-b793-f33dfdee85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_json(smiles_list, batch_size=1):\n",
    "    batches = [smiles_list[i:i + batch_size] for i in range(0, len(smiles_list), batch_size)]\n",
    "    json_data = {\"batches\": batches}\n",
    "    return json_data\n",
    "\n",
    "def get_descriptors(json_data, url=\"http://192.168.1.100:80/predict\"):\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    response = requests.post(url, data=json.dumps(json_data), headers=headers)\n",
    "    response_json = response.json()\n",
    "    response_dict = json.loads(response_json[\"Prediction\"])\n",
    "    df = pd.DataFrame.from_dict(response_dict)\n",
    "    descriptor_list = df.iloc[:, 2:].values\n",
    "    return descriptor_list\n",
    "\n",
    "def get_cddd(smiles_list, batch_size=1):\n",
    "    smiles_list = list(smiles_list)\n",
    "    json_data = prepare_json(smiles_list)\n",
    "    descriptor_list = get_descriptors(json_data)\n",
    "    return descriptor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ece9d-ce1b-4a70-a778-7c9d5e8dad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_cddd(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cea816-5e3d-48a6-b834-a2a8730f210d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Molecular Transformer Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b5cdf-5f4d-4234-8dff-b3f8cf73b7d7",
   "metadata": {},
   "source": [
    "The embedding can be calculated using the scripts accessible [here](https://github.com/mpcrlab/MolecularTransformerEmbeddings). Then the generated .npz file can be ocnverted to embedding list using the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb786933-d997-490b-8245-a9a07f1870e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mte(file_path, smiles_list):\n",
    "    embedding_npz = np.load(file_path)\n",
    "    embedding_list = []\n",
    "    for smiles in smiles_list:\n",
    "        embedding = np.mean(embedding_npz[smiles], axis=0)\n",
    "        embedding_list.append(embedding)\n",
    "    embedding_list = np.array(embedding_list)\n",
    "    return embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d4c25d-586f-4bfd-a79d-8d87262eec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_mte(\"data/embedding/jak2_smiles.npz\", smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df5454-381a-40b1-9719-5bccd869f7c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MACAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e46776-4808-4f7e-8b3e-426eecbdc0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from macaw import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85136c8-5dcf-4277-9ccb-ad946c37a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_macaw(smiles_list, n_dimensions=20):\n",
    "    mcw = MACAW(n_components=n_dimensions)\n",
    "    mcw.fit(smiles_list)\n",
    "    embedding = mcw.transform(smiles_list)\n",
    "    retur embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e7ee91-0d7f-43ef-8334-3fc4ca2f856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_macaw(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30519744-3eff-47b1-be99-ed9c57b49e1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MolFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13dc556d-aae8-4f2d-977e-bc203e901946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoModelForMaskedLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model = AutoModel.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", deterministic_eval=True, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970dc050-2f43-4652-ae57-bc76eceb3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molformer(smiles_list):\n",
    "    inputs = tokenizer(smiles_list, padding=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embedding = np.array(outputs.pooler_output)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545255a1-33fd-47ed-8e20-0dd397bcd92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_molformer(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e919b7-3345-48af-a345-0f11df37e4c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ad638f-0660-4471-9e8f-713e1e708174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"entropy/gpt2_zinc_87m\", max_len=256)\n",
    "model = GPT2LMHeadModel.from_pretrained('entropy/gpt2_zinc_87m')\n",
    "collator = DataCollatorWithPadding(tokenizer, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f859a7-c232-4f91-90b7-09fef505698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt2(smiles_list):\n",
    "    embedding_list = []\n",
    "    for i in range(0, len(smiles_list), 100):\n",
    "        print(f\"{i}:{i+100}\")\n",
    "        inputs = collator(tokenizer(smiles_list[i:i+100]))\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        full_embeddings = outputs[-1][-1]\n",
    "        mask = inputs['attention_mask']\n",
    "        embeddings = ((full_embeddings * mask.unsqueeze(-1)).sum(1) / mask.sum(-1).unsqueeze(-1))\n",
    "        embedding_list.extend(embeddings)\n",
    "    for i, vec in enumerate(embedding_list):\n",
    "        embedding_list[i] = vec.detach().numpy()\n",
    "    return embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751fa95-df9d-4af5-b911-538e1bdcd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_gpt2(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7badf9af-41d6-450d-9377-cf2750a9ff41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## BERT for SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a859abe-f19c-466a-9f7a-c605f1a487fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"unikei/bert-base-smiles\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"unikei/bert-base-smiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1ebd01be-dc9a-47b5-9585-26d178daf154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_smiles(smiles_list):\n",
    "    inputs = tokenizer(smiles_list, padding=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.detach().numpy()\n",
    "    embedding = np.mean(embedding, axis=1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0fe199-182b-48d0-ade5-c78deca9e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_bert_smiles(smiles_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401bbee-9573-4565-8cb0-f43b82fcc8fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Distance and correlation calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f9c76-73a9-4d5e-b3dc-3f49f974981f",
   "metadata": {},
   "source": [
    "Embeddings should be 2-dimensional arrays saved as .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d49b624-d892-48eb-8b0e-b13fb3552d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fingerprints(smiles_list):\n",
    "    fpgen = rdGen.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "    fingerprint_list = np.array([fpgen.GetFingerprint(Chem.MolFromSmiles(smiles)) for smiles in smiles_list], dtype=rdkit.DataStructs.cDataStructs.ExplicitBitVect)\n",
    "    return fingerprint_list\n",
    "\n",
    "def get_pairwise_similarity(smiles_list):    \n",
    "    fingerprint_list = get_fingerprints(smiles_list)\n",
    "    similarity_matrix = 1 - pairwise_distances(X = fingerprint_list, metric='jaccard', n_jobs = -1)\n",
    "    return similarity_matrix\n",
    "\n",
    "def get_pairwise_distance(vec_list, metric):\n",
    "    distance_matrix = pairwise_distances(X=vec_list, metric=metric, n_jobs = -1)\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e302f3-3cdc-4bb2-9671-06646dcab527",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list = np.genfromtxt(\"data/jak2_smiles.txt\", dtype=str, comments=None)\n",
    "print(f\"Loaded {len(smiles_list)} SMILES\")\n",
    "\n",
    "similarity_matrix = get_pairwise_similarity(smiles_list)\n",
    "with open(f\"data/distance/similarity_jak2.pkl\", \"wb\") as file:\n",
    "    pickle.dump(similarity_matrix, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143d32d-0cac-4b60-abbd-60e762e7b97f",
   "metadata": {},
   "source": [
    "Calculate distances with different measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5254e-8e41-4720-a794-81ade36889cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = (\n",
    "    (\"mol2vec\", get_mol2vec),\n",
    "    (\"graph2vec\", get_graph2vec),\n",
    "    (\"chemberta\", get_chemberta),\n",
    "    (\"cddd\", get_cddd),\n",
    "    (\"mte\", get_mte),\n",
    "    (\"macaw\", get_macaw),\n",
    "    (\"molformer\", get_molformer),\n",
    "    (\"gpt2\", get_mol2vec),\n",
    "    (\"bert_smiles\", get_bert_smiles)\n",
    ")\n",
    "measure_list = (\"euclidean\", \"cosine\", \"canberra\")\n",
    "\n",
    "for emb_name, emb_func in embedding_list:\n",
    "    embedding = emb_func(smiles_list)\n",
    "    embedding = np.nan_to_num(embedding)\n",
    "    print(f\"Calculated {emb_name} embedding: {embedding.shape}\")\n",
    "    with open(f\"data/embedding/{emb_name}_embedding.pkl\", \"wb\") as file:\n",
    "        pickle.dump(similarity_matrix, file)\n",
    "\n",
    "    for m_name in measure_list:\n",
    "        distance_matrix = get_pairwise_distance(embedding, m_name)\n",
    "        with open(f\"data/distance/distance_{emb_name}_{m_name}.pkl\", \"wb\") as file:\n",
    "            pickle.dump(distance_matrix, file)\n",
    "        print(f\"\\tDistance {m_name} - done.\")\n",
    "    print(f\"Distance {emb_name} - done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df4508-d7af-45c0-a56d-74c5bba4b121",
   "metadata": {},
   "source": [
    "Calculate correlation between embedding distance and Morgan Tanimoto similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1617de5e-a3d3-418b-97c4-92333f6b7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = (\"mol2vec\", \"graph2vec\", \"chemberta\", \"cddd\", \"mte\", \"macaw\", \"molformer\", \"gpt2\", \"bert_smiles\")\n",
    "measures_list = (\"euclidean\", \"cosine\", \"canberra\")\n",
    "\n",
    "with open(f\"data/distance/similarity_jak2.pkl\", \"rb\") as file:\n",
    "    similarity_matrix = pickle.load(file)[:1000,:1000]\n",
    "print(similarity_matrix.shape)\n",
    "    \n",
    "similarity_matrix_flat = similarity_matrix.flatten()\n",
    "X = [similarity_matrix_flat]\n",
    "for measure in measures_list:\n",
    "    print(measure)\n",
    "    with open(f\"data/distance/distance_{embedding}_{measure}.pkl\", \"rb\") as file:\n",
    "        dist_matrix = pickle.load(file)\n",
    "    dist_matrix = np.nan_to_num(dist_matrix[:5000, :5000])\n",
    "    dist_matrix = dist_matrix.flatten()\n",
    "    dist_matrix_norm = (dist_matrix - np.min(dist_matrix))/np.ptp(dist_matrix)\n",
    "    X.append(dist_matrix_norm)\n",
    "corr_matrix = np.corrcoef(X)\n",
    "\n",
    "print(corr_matrix)\n",
    "for measure, corr in zip(measures_list, corr_matrix[0, 1:]):\n",
    "    print(f\"{measure}: {corr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
